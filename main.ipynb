{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1729149620190,
     "user": {
      "displayName": "Olivier Jaylet",
      "userId": "17139053766533794240"
     },
     "user_tz": -300
    },
    "id": "eTiJKDRPOrsY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Benchmark Model\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import BigramTagger\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Home-made functions\n",
    "#from functions import *\n",
    "from fn_feature import *\n",
    "from fn_nltk import *\n",
    "from fn_results import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_kz = 'data/kdt-NLANU-0.01.connlu.txt'\n",
    "path_en = 'data/en_ewt-ud-dev.conllu'\n",
    "#path_tu = 'data/tr_kenet-ud-dev.conllu'\n",
    "\n",
    "path_data = [\n",
    "    path_kz, \n",
    "    path_en, \n",
    "#    path_tu\n",
    "    ]\n",
    "\n",
    "languages = [\n",
    "    'kazakh',\n",
    "    'english',\n",
    "#    'turkish'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ID\", \"WORD\", \"LEMMA\", \"POS\", \"XPOS\", \"MORPH\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n",
    "\n",
    "for path, lang in tqdm(zip(path_data, languages)) :\n",
    "\n",
    "    print(\"____________________________\" , lang.upper(), \"CORPUS ____________________________\")\n",
    "\n",
    "\n",
    "    # Read the file and convert it to a DataFrame\n",
    "    df = pd.read_csv(path,\n",
    "                    sep='\\t',\n",
    "                    names=columns,\n",
    "                    skip_blank_lines=True\n",
    "                    )\n",
    "\n",
    "    # run the hand-made function to clean data\n",
    "    df = clean_data(df)\n",
    "\n",
    "    tagged_sentences = data_to_nltk(df)\n",
    "\n",
    "    X_lex, Y_lex = get_values(df)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = train_test_split(tagged_sentences, \n",
    "                                            test_size=0.1, \n",
    "                                            random_state=42\n",
    "                                            )\n",
    "\n",
    "\n",
    "    ################## NLTK MODEL ##################\n",
    "    # setup and train BigramTagger\n",
    "    DFTagger = DefaultTagger(\"NN\")\n",
    "    Tagger = BigramTagger(train_data, \n",
    "                        backoff=DFTagger)\n",
    "\n",
    "    # Extract true labels and predicted labels from the test data\n",
    "    Y_test_nltk, predicts_test_nltk = extract_tags(test_data, Tagger)\n",
    "    Y_train_nltk, predicts_train_nltk = extract_tags(train_data, Tagger)\n",
    "    ################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_test, y_test = extract_words_and_tags(test_data)\n",
    "    X_train, y_train = extract_words_and_tags(train_data)\n",
    "\n",
    "    #get max word length\n",
    "    max_word_len = max(max([len(w) for w in Y_lex]), max([len(w) for w in X_lex]))\n",
    "\n",
    "    #Char2vec model\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, \n",
    "                                analyzer='char'\n",
    "                                )\n",
    "\n",
    "    X = vectorizer.fit_transform(X_lex)\n",
    "    dic = vectorizer.get_feature_names_out() # letter dictionary\n",
    "    num_letters = len(dic)\n",
    "    mx = X.T.dot(X) # letter cooccurence matrix\n",
    "    mx = mx.toarray()\n",
    "\n",
    "    #Vectorize X only\n",
    "    X_lex_vec_train = [alpha_vec2(w, mx, max_word_len, dic) for w in X_train]\n",
    "    X_lex_vec_test = [alpha_vec2(w, mx, max_word_len, dic) for w in X_test]\n",
    "\n",
    "    # Encode Y\n",
    "    list_tags = list_all_POS_tags(y = y_train)\n",
    "    encoder_tag = LabelEncoder().fit(list_tags)\n",
    "\n",
    "    Y_train = encoder_tag.transform(y_train)\n",
    "    Y_test = encoder_tag.transform(y_test)\n",
    "\n",
    "    # Build & train model\n",
    "    best_model = ExtraTreesClassifier(n_estimators=10,\n",
    "                                    n_jobs=-1,\n",
    "                                    criterion='entropy',\n",
    "                                    bootstrap=True\n",
    "                                    )\n",
    "\n",
    "    best_model.fit(X_lex_vec_train, Y_train)\n",
    "\n",
    "    # predict both train and test sets\n",
    "    predicts_test = best_model.predict(X_lex_vec_test)\n",
    "    predicts_train = best_model.predict(X_lex_vec_train)\n",
    "\n",
    "    \n",
    "    #####################################################################################################\n",
    "    ########################################## result analysis ##########################################\n",
    "    #####################################################################################################\n",
    "    test_acc, test_f1, test_recall, train_acc, train_f1, train_recall = calculate_results(Y_test, \n",
    "                                                                                            Y_train, \n",
    "                                                                                            predicts_test, \n",
    "                                                                                            predicts_train\n",
    "                                                                                            )\n",
    "    test_acc_nltk, test_f1_nltk, test_recall_nltk, train_acc_nltk, train_f1_nltk, train_recall_nltk = calculate_results(Y_test_nltk, \n",
    "                                                                                            Y_train_nltk, \n",
    "                                                                                            predicts_test_nltk, \n",
    "                                                                                            predicts_train_nltk\n",
    "                                                                                            )\n",
    "\n",
    "    data = {\n",
    "        \"Metric\": [\"Test Accuracy\", \"Test F1 Score\", \"Test Recall\", \n",
    "                \"Train Accuracy\", \"Train F1 Score\", \"Train Recall\"],\n",
    "        \"Multi-language POS Tagger\": [test_acc, test_f1, test_recall,\n",
    "                        train_acc, train_f1, train_recall],\n",
    "        \"NLTK POS Tagger\": [test_acc_nltk, test_f1_nltk, test_recall_nltk,\n",
    "                    train_acc_nltk, train_f1_nltk, train_recall_nltk]\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_results = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    display(df_results)\n",
    "\n",
    "    \n",
    "    fig = plot_confusion_matrix(Y_test, predicts_test, list_tags, 'Test set', lang)\n",
    "    save_graph_to_folder(fig, lang, 'confusion_matrix_test')\n",
    "    \n",
    "    fig = plot_confusion_matrix(Y_train, predicts_train, list_tags, 'Train set', lang)\n",
    "    save_graph_to_folder(fig, lang, 'confusion_matrix_train')\n",
    "\n",
    "    df_tag_acc = per_tag_accuracy(Y_test, \n",
    "                              predicts_test, \n",
    "                              list_tags, \n",
    "                              encoder_tag\n",
    "                              )\n",
    "\n",
    "    display(df_tag_acc) # display accuracy per Tag\n",
    "    \n",
    "    df_tag_dist = tag_prediction_nb(\n",
    "        Y_test, \n",
    "        predicts_test, \n",
    "        list_tags, \n",
    "        encoder_tag\n",
    "        )\n",
    "\n",
    "    display(df_tag_dist) # display the number of correct and incorect predictions for each tag\n",
    "\n",
    "\n",
    "    fig = plot_dist_predictions(df_tag_dist,\n",
    "                                lang)\n",
    "    save_graph_to_folder(fig, lang, 'dist_predictions')\n",
    "    \n",
    "    \n",
    "    mistake_freq_df = mistake_frequency_by_word_type(Y_test, \n",
    "                                                    predicts_test, \n",
    "                                                    list_tags, \n",
    "                                                    encoder_tag\n",
    "                                                    )\n",
    "\n",
    "    display(mistake_freq_df.head(n=10)) # Print 10 most frequent errors"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
