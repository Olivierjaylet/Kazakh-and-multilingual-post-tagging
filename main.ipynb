{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":317,"status":"ok","timestamp":1729149620190,"user":{"displayName":"Olivier Jaylet","userId":"17139053766533794240"},"user_tz":-300},"id":"eTiJKDRPOrsY"},"outputs":[],"source":["import pandas as pd\n","\n","from tqdm import tqdm\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import  train_test_split\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","from functions import *\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["## Import & clean data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["path_kz = 'data/kdt-NLANU-0.01.connlu.txt'\n","path_en = 'data/en_ewt-ud-dev.conllu'\n","path_tu = 'data/tr_kenet-ud-dev.conllu'\n","\n","path_data = [\n","    path_kz, \n","    path_en, \n","    path_tu\n","    ]\n","\n","languages = [\n","    'kazakh',\n","    'english',\n","    'turkish'\n","    ]\n","\n","dic_ = {}\n","for l in languages : \n","    dic_[l] = {\n","            'test_acc' : '',\n","            'test_f1' : '',\n","            'train_acc' : '',\n","            \"train_f1\" : '',\n","            \"Y\" : '',\n","            \"predicts\" : '', \n","            \"list_tags\" : ''\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["columns = [\"ID\", \"WORD\", \"LEMMA\", \"POS\", \"XPOS\", \"MORPH\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n","\n","for path, lang in tqdm(zip(path_data, languages)) :\n","\n","    print(\"____________________________\" , lang.upper(), \"CORPUS ____________________________\")\n","\n","    # Read the file and convert it to a DataFrame\n","    df = pd.read_csv(path,\n","                    sep='\\t',\n","                    names=columns,\n","                    skip_blank_lines=True\n","                    )\n","\n","    # run the hand-made function to clean data\n","    X_lex, Y_lex = clean_data(df)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X_lex, \n","                                                        Y_lex, \n","                                                        test_size=0.1, \n","                                                        random_state=42\n","                                                        )\n","\n","    #get max word length\n","    max_word_len = max(max([len(w) for w in Y_lex]), max([len(w) for w in X_lex]))\n","\n","    #Char2vec model\n","    vectorizer = TfidfVectorizer(lowercase=False, \n","                                analyzer='char'\n","                                )\n","\n","    X = vectorizer.fit_transform(X_lex)\n","    dic = vectorizer.get_feature_names_out() # letter dictionary\n","    num_letters = len(dic)\n","    mx = X.T.dot(X) # letter cooccurence matrix\n","    mx = mx.toarray()\n","\n","    #Vectorize X only\n","    X_lex_vec_train = [alpha_vec2(w, mx, max_word_len, dic) for w in X_train]\n","    X_lex_vec_test = [alpha_vec2(w, mx, max_word_len, dic) for w in X_test]\n","\n","    # Encode Y\n","    list_tags = list_all_POS_tags(y = y_train)\n","    encoder_tag = LabelEncoder().fit(list_tags)\n","\n","    Y_train = encoder_tag.transform(y_train)\n","    Y_test = encoder_tag.transform(y_test)\n","\n","    # Build & train model\n","    best_model = ExtraTreesClassifier(n_estimators=10,\n","                                    n_jobs=-1,\n","                                    criterion='entropy',\n","                                    bootstrap=True\n","                                    )\n","\n","    best_model.fit(X_lex_vec_train, Y_train)\n","\n","    # predict both train and test sets\n","    predicts_test = best_model.predict(X_lex_vec_test)\n","    predicts_train = best_model.predict(X_lex_vec_train)\n","    \n","    test_acc, test_f1, train_acc, train_f1 = calculate_results(Y_test, \n","                      Y_train, \n","                      predicts_test, \n","                      predicts_train\n","                      )\n","    print(\"Test Accuracy:\", round(test_acc, 3))\n","    print(\"Test F1 Score:\", round(test_f1, 3))\n","    print(\"Train Accuracy:\", round(train_acc, 3))\n","    print(\"Train F1 Score:\", round(train_f1, 3))\n","\n","\n","    fig = plot_confusion_matrix(Y_test, predicts_test, list_tags, 'Test set', lang)\n","    save_graph_to_folder(fig, lang, 'confusion_matrix_test')\n","    \n","    fig = plot_confusion_matrix(Y_train, predicts_train, list_tags, 'Train set', lang)\n","    save_graph_to_folder(fig, lang, 'confusion_matrix_train')\n","\n","\n","    df_tag_acc = per_tag_accuracy(Y_test, \n","                              predicts_test, \n","                              list_tags, \n","                              encoder_tag\n","                              )\n","\n","    display(df_tag_acc) # display accuracy per Tag\n","    \n","    df_tag_dist = tag_prediction_nb(\n","        Y_test, \n","        predicts_test, \n","        list_tags, \n","        encoder_tag\n","        )\n","\n","    display(df_tag_dist) # display the number of correct and incorect predictions for each tag\n","\n","\n","    fig = plot_dist_predictions(df_tag_dist,\n","                                lang)\n","    save_graph_to_folder(fig, lang, 'dist_predictions')\n","    \n","    \n","    mistake_freq_df = mistake_frequency_by_word_type(Y_test, \n","                                                    predicts_test, \n","                                                    list_tags, \n","                                                    encoder_tag\n","                                                    )\n","\n","    display(mistake_freq_df.head(n=10)) # Print 10 most frequent errors\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
